_commit_hash ---> None
_name_or_path ---> 
act_dim ---> 6
action_tanh ---> True
activation_function ---> relu
add_cross_attention ---> False
architectures ---> None
attn_pdrop ---> 0.1
attribute_map ---> {'max_position_embeddings': 'n_positions', 'num_attention_heads': 'n_head', 'num_hidden_layers': 'n_layer'}
bad_words_ids ---> None
begin_suppress_tokens ---> None
bos_token_id ---> 50256
chunk_size_feed_forward ---> 0
cross_attention_hidden_size ---> None
decoder_start_token_id ---> None
diversity_penalty ---> 0.0
do_sample ---> False
early_stopping ---> False
embd_pdrop ---> 0.1
encoder_no_repeat_ngram_size ---> 0
eos_token_id ---> 50256
exponential_decay_length_penalty ---> None
finetuning_task ---> None
forced_bos_token_id ---> None
forced_eos_token_id ---> None
hidden_size ---> 128
id2label ---> {0: 'LABEL_0', 1: 'LABEL_1'}
initializer_range ---> 0.02
is_composition ---> False
is_decoder ---> False
is_encoder_decoder ---> False
keys_to_ignore_at_inference ---> ['past_key_values']
label2id ---> {'LABEL_0': 0, 'LABEL_1': 1}
layer_norm_epsilon ---> 1e-05
length_penalty ---> 1.0
max_ep_len ---> 4096
max_length ---> 20
min_length ---> 0
model_type ---> decision_transformer
n_head ---> 1
n_inner ---> None
n_layer ---> 3
n_positions ---> 1024
name_or_path ---> 
no_repeat_ngram_size ---> 0
num_beam_groups ---> 1
num_beams ---> 1
num_labels ---> 2
num_return_sequences ---> 1
output_attentions ---> False
output_hidden_states ---> False
output_scores ---> False
pad_token_id ---> None
prefix ---> None
problem_type ---> None
pruned_heads ---> {}
remove_invalid_values ---> False
reorder_and_upcast_attn ---> False
repetition_penalty ---> 1.0
resid_pdrop ---> 0.1
return_dict ---> True
return_dict_in_generate ---> False
scale_attn_by_inverse_layer_idx ---> False
scale_attn_weights ---> True
sep_token_id ---> None
state_dim ---> 17
suppress_tokens ---> None
task_specific_params ---> None
temperature ---> 1.0
tf_legacy_loss ---> False
tie_encoder_decoder ---> False
tie_word_embeddings ---> True
tokenizer_class ---> None
top_k ---> 50
top_p ---> 1.0
torch_dtype ---> None
torchscript ---> False
transformers_version ---> None
typical_p ---> 1.0
use_bfloat16 ---> False
use_cache ---> True
use_return_dict ---> True
vocab_size ---> 1


from_dict ---> <bound method PretrainedConfig.from_dict of <class 'transformers.models.cnn_decision_transformer.configuration_decision_transformer.DecisionTransformerConfig'>>
from_json_file ---> <bound method PretrainedConfig.from_json_file of <class 'transformers.models.cnn_decision_transformer.configuration_decision_transformer.DecisionTransformerConfig'>>
from_pretrained ---> <bound method PretrainedConfig.from_pretrained of <class 'transformers.models.cnn_decision_transformer.configuration_decision_transformer.DecisionTransformerConfig'>>
get_config_dict ---> <bound method PretrainedConfig.get_config_dict of <class 'transformers.models.cnn_decision_transformer.configuration_decision_transformer.DecisionTransformerConfig'>>
register_for_auto_class ---> <bound method PretrainedConfig.register_for_auto_class of <class 'transformers.models.cnn_decision_transformer.configuration_decision_transformer.DecisionTransformerConfig'>>
