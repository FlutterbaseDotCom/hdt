{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### V 17:54"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpjcCHxkeLKO"
      },
      "source": [
        "### Install initial environment in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRx1SfZkeLKQ",
        "outputId": "7cb88885-ea76-40bf-db14-2c9cdcb1373d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  if not os.path.exists('/content/.already_installed'):\n",
        "    !git clone https://github.com/FlutterbaseDotCom/hdt\n",
        "    !apt-get install -y swig\n",
        "    !pip install -r requirements.txt\n",
        "    with open('/content/.already_installed', 'w') as f:\n",
        "        f.write('done')\n",
        "  %cd /content/hdt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyFxDs1beLKR"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99O1Dp3weLKR",
        "outputId": "caaf2cfe-d075-495a-857b-fa23d9426e62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacob/.pyenv/versions/3.10.12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from datasets import Dataset, load_dataset\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.torch_layers import NatureCNN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "from torch.utils.data import Subset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "from cnn_decision_transformer.configuration import DecisionTransformerConfig\n",
        "from cnn_decision_transformer.cnn_decision_transformer_model import DecisionTransformerModel\n",
        "from cnn_decision_transformer.cnn_feature_extractor import prepare_observation_array\n",
        "from cnn_decision_transformer.cnn_decision_transformer_trainable import DecisionTransformerGymDataCollator, TrainableDT\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4p2pRjAeLKS"
      },
      "source": [
        "### Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhM2DabQeLKT",
        "outputId": "21e5d08d-53d4-4b2c-aa0d-9ff8d77d431b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacob/.pyenv/versions/3.10.12/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: Can't get attribute '_make_function' on <module 'cloudpickle.cloudpickle' from '/Users/jacob/.pyenv/versions/3.10.12/lib/python3.10/site-packages/cloudpickle/cloudpickle.py'>\n",
            "  warnings.warn(\n",
            "/Users/jacob/.pyenv/versions/3.10.12/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:166: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: Can't get attribute '_make_function' on <module 'cloudpickle.cloudpickle' from '/Users/jacob/.pyenv/versions/3.10.12/lib/python3.10/site-packages/cloudpickle/cloudpickle.py'>\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 22.25806451612903 episodes steps: 100\n",
            "Episode: 1 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 16.49006622516558 episodes steps: 100\n",
            "Episode: 2 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 6.835016835016857 episodes steps: 100\n",
            "Episode: 3 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 9.455252918287968 episodes steps: 100\n",
            "Episode: 4 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 23.003300330033 episodes steps: 100\n",
            "Episode: 5 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 14.734982332155512 episodes steps: 100\n",
            "Episode: 6 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 17.777777777777786 episodes steps: 100\n",
            "Episode: 7 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 17.30375426621162 episodes steps: 100\n",
            "Episode: 8 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 18.846153846153847 episodes steps: 100\n",
            "Episode: 9 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 14.390243902439058 episodes steps: 100\n",
            "Episode: 10 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 17.777777777777793 episodes steps: 100\n",
            "Episode: 11 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 26.231884057970994 episodes steps: 100\n",
            "Episode: 12 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 15.925925925925961 episodes steps: 100\n",
            "Episode: 13 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 10.066889632107058 episodes steps: 100\n",
            "Episode: 14 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 12.222222222222253 episodes steps: 100\n",
            "Episode: 15 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 21.14186851211071 episodes steps: 100\n",
            "Episode: 16 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 15.641025641025667 episodes steps: 100\n",
            "Episode: 17 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 17.700831024930764 episodes steps: 100\n",
            "Episode: 18 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 11.897810218978139 episodes steps: 100\n",
            "Episode: 19 of 20:\n",
            "....................................................................................................\n",
            "Total reward: 17.86377708978329 episodes steps: 100\n"
          ]
        }
      ],
      "source": [
        "from utils.config import MAX_EPISODE_STEPS, NUM_EPISODES, RTG_GAMMA\n",
        "\n",
        "\n",
        "env =  gym.make('CarRacing-v2', continuous=False) #, render_mode='human'\n",
        "model = DQN\n",
        "tmp_model_path ='./models/dql_pretrained/dql_rl_11.zip'\n",
        "loaded_model = model.load(tmp_model_path)\n",
        "\n",
        "features = {\n",
        "    \"observations\": [],\n",
        "    \"actions\": [],\n",
        "    \"rewards\": [],\n",
        "    \"dones\": [],\n",
        "    \"rtg\": []\n",
        "}\n",
        "\n",
        "def calculate_rtg(rewards, gamma):\n",
        "    rtg = np.zeros_like(rewards)\n",
        "    rtg[-1] = rewards[-1]\n",
        "    for i in reversed(range(len(rewards) - 1)):\n",
        "        rtg[i] = rewards[i] + gamma * rtg[i + 1]\n",
        "    return rtg\n",
        "\n",
        "for episode in range(NUM_EPISODES):\n",
        "    print(f\"Episode: {episode} of {NUM_EPISODES}:\" )\n",
        "    [obs, _] = env.reset()\n",
        "    done = False\n",
        "\n",
        "    o, a, r, d, g = [], [], [], [], []\n",
        "    total_reward = 0\n",
        "    sti = 0\n",
        "    while not done:\n",
        "        sti = sti + 1\n",
        "        if sti > MAX_EPISODE_STEPS:\n",
        "            break\n",
        "\n",
        "        # if random.random() < epsilon:\n",
        "        #     action = 3# env.action_space.sample()\n",
        "        # else:\n",
        "        action, _states = loaded_model.predict(obs,deterministic=True)\n",
        "        new_obs, reward, done, t, i = env.step(action)\n",
        "        total_reward = total_reward + reward\n",
        "        oarr = prepare_observation_array(obs)\n",
        "        o.append(oarr.flatten())\n",
        "        a.append(action.item())\n",
        "        r.append(reward)\n",
        "        d.append(done)\n",
        "        obs = new_obs\n",
        "        print(\".\", end=\"\")\n",
        "\n",
        "        # check if last 50 steps does not contain a single positive reward\n",
        "        if len(r) > 100 and max(r[-50:]) <= 0:\n",
        "            # cut last 50 and set done to True\n",
        "            r = r[:-50]\n",
        "            d = d[:-50]\n",
        "            a = a[:-50]\n",
        "            d[-1] = True\n",
        "            print('\\nstopping due to the last 50 steps not negative rewards')\n",
        "            break\n",
        "    print(f\"\\nTotal reward: {total_reward} episodes steps: {len(o)}\")\n",
        "\n",
        "    features[\"observations\"].append(o)\n",
        "    features[\"actions\"].append(a)\n",
        "    features[\"rewards\"].append(r)\n",
        "    features[\"dones\"].append(d)\n",
        "    episode_rtg = calculate_rtg(r, RTG_GAMMA)\n",
        "    features[\"rtg\"].append(episode_rtg)\n",
        "\n",
        "env.close()\n",
        "#print(len(features[\"actions\"]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to_hdf5(features, './temp_data/features_unk.hdf5', use_compression=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1 = load_from_hdf5('./temp_data/features.hdf5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCAF3Q2meLKU"
      },
      "source": [
        "### Persist Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UqCjWIr2eLKU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27648"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#dataset = Dataset.from_dict(features)\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, src):\n",
        "        self.size = len(src[\"observations\"])  # Assuming all lists are the same length\n",
        "        self.src = src\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitems__(self, index):\n",
        "        return [self._item(i) for i in index]\n",
        "\n",
        "    def _item(self, idx):\n",
        "        # It is better to ensure this is an internal method used within the class only.\n",
        "        if isinstance(idx, str):\n",
        "            return self.src[idx]\n",
        "        \n",
        "        return {\n",
        "            \"observations\": self.src[\"observations\"][idx],\n",
        "            \"actions\": self.src[\"actions\"][idx], \n",
        "            \"rewards\": self.src[\"rewards\"][idx],\n",
        "            \"dones\": self.src[\"dones\"][idx],\n",
        "            \"rtg\": self.src[\"rtg\"][idx]\n",
        "        }\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Here, we use 'index' instead of 'i'\n",
        "        return self._item(index)\n",
        "\n",
        "feature_dataset = FeatureDataset(src=features)\n",
        "len(feature_dataset[0][\"observations\"][0])\n",
        "#dataset.save_to_disk('datasets/car_racing_0070_0045/')\n",
        "\n",
        "# dataset_size = len(dataset)\n",
        "# split_point = int(0.9 * dataset_size)\n",
        "# #dataset is already shuffled\n",
        "# train_dataset = Subset(dataset, range(split_point))\n",
        "# val_dataset = Subset(dataset, range(split_point, dataset_size))\n",
        "\n",
        "#from datasets import load_from_disk\n",
        "#dataset = load_from_disk('datasets/car_racing_002/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "hSYgW1lWeLKU",
        "outputId": "069df814-9e81-41ec-81f9-379007a372ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myakiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/jacob/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/jacob/Documents/T/hdt/wandb/run-20231106_142558-dt_20</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/yakiv/CarRacingDT/runs/dt_20' target=\"_blank\">DT_20</a></strong> to <a href='https://wandb.ai/yakiv/CarRacingDT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/yakiv/CarRacingDT' target=\"_blank\">https://wandb.ai/yakiv/CarRacingDT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/yakiv/CarRacingDT/runs/dt_20' target=\"_blank\">https://wandb.ai/yakiv/CarRacingDT/runs/dt_20</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from utils.config import CONFIG, WANDB_ID, WNDB_NAME\n",
        "# TOML-formatted string\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = 'cnn_decision_transformer.ipynb'\n",
        "os.environ['WANDB_MODE']='online'\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
        "\n",
        "\n",
        "wandb.login(key=\"f060d3284088ffaf4624e2de8b236f39711a99a2\")\n",
        "wandb.init(resume=WANDB_ID,\n",
        "           name = WNDB_NAME,\n",
        "           mode=\"online\",\n",
        "           entity=\"yakiv\",\n",
        "            project=\"CarRacingDT\",\n",
        "            #resume= \"allow\"\n",
        "            config=CONFIG\n",
        "           )\n",
        "\n",
        "\n",
        "env =  gym.make('CarRacing-v2', continuous=False) #, render_mode='human'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmsghw2HeLKV"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "2HyHbWANeLKV",
        "outputId": "12b2da30-ef13-401a-88ae-c59cf2ee5f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'action_num': 6, 'state_dim': 27648, 'act_dim': 1, 'hidden_size': 128, 'max_ep_len': 1000, 'action_tanh': True, 'vocab_size': 1, 'n_positions': 1024, 'n_layer': 3, 'n_head': 1, 'n_inner': None, 'activation_function': 'relu', 'resid_pdrop': 0.1, 'embd_pdrop': 0.1, 'attn_pdrop': 0.1, 'layer_norm_epsilon': 1e-05, 'initializer_range': 0.02, 'scale_attn_weights': True, 'use_cache': True, 'scale_attn_by_inverse_layer_idx': False, 'reorder_and_upcast_attn': False, 'bos_token_id': 50256, 'eos_token_id': 50256, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 10, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'pad_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.34.1', 'model_type': 'decision_transformer'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8, 0, 9, 1, 4, 5, 6, 3]\n",
            "Collator sz:8: started at 14:38:52\n",
            "Collator sz:8: finished at 14:38:52 and took 15ms\n",
            "[2, 7]\n",
            "Collator sz:2: started at 14:38:52\n",
            "Collator sz:2: finished at 14:38:52 and took 2ms\n",
            "DecisionTransformerModel.forward batch on device: mps:0 sz:8 seq len: 10 : started at 14:38:52\n",
            "DecisionTransformerModel.forward batch on device: mps:0 sz:8 seq len: 10 : finished at 14:38:52 and took 565ms\n",
            "Trainable Forward pass: started at 14:38:52\n",
            "loss: 1.6918296813964844\n",
            "Trainable Forward pass: finished at 14:38:52 and took 129ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:01<00:01,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTransformerModel.forward batch on device: mps:0 sz:2 seq len: 10 : started at 14:38:54\n",
            "DecisionTransformerModel.forward batch on device: mps:0 sz:2 seq len: 10 : finished at 14:38:54 and took 364ms\n",
            "Trainable Forward pass: started at 14:38:54\n",
            "loss: 1.633987307548523\n",
            "Trainable Forward pass: finished at 14:38:54 and took 59ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 2.9874, 'train_samples_per_second': 3.347, 'train_steps_per_second': 0.669, 'train_loss': 1.6629085540771484, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=1.6629085540771484, metrics={'train_runtime': 2.9874, 'train_samples_per_second': 3.347, 'train_steps_per_second': 0.669, 'train_loss': 1.6629085540771484, 'epoch': 1.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "collator = DecisionTransformerGymDataCollator(feature_dataset, max_len=CONFIG[\"max_length\"],   max_ep_len=CONFIG[\"max_ep_len\"],)\n",
        "\n",
        "dt_config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim,\n",
        "                                      max_length = CONFIG[\"max_length\"],\n",
        "                                      max_ep_len = CONFIG[\"max_ep_len\"],  \n",
        "                                      )\n",
        "print(dt_config.to_dict())\n",
        "\n",
        "model = TrainableDT(dt_config)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output/\",\n",
        "    report_to=\"wandb\",\n",
        "    save_steps=CONFIG[\"save_steps\"],\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
        "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
        "    learning_rate=CONFIG[\"learning_rate\"],\n",
        "    weight_decay=CONFIG[\"weight_decay\"],\n",
        "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
        "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
        "    logging_steps=CONFIG[\"LOG_INTERVAL\"],\n",
        ")\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.arr = [0] * size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    def __getitems__(self, index):\n",
        "        print(index)\n",
        "        return index\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        print(index)\n",
        "        return index\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=DummyDataset(len(feature_dataset)),\n",
        "    data_collator=collator,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#play\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display as ipy_display, clear_output\n",
        "#import gymnasium as gym\n",
        "# build the environment\n",
        "max_ep_len = 1000\n",
        "device = 'cpu'\n",
        "model = model.to('cpu')\n",
        "scale = 1000.0  # normalization for rewards/returns\n",
        "TARGET_RETURN = 900 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
        "\n",
        "env =  gym.make('CarRacing-v2', render_mode='rgb_array', continuous=False) #, \n",
        "\n",
        "state_dim = 96*96*3\n",
        "act_dim = 1\n",
        "# Create the decision transformer model\n",
        "\n",
        "# Interact with the environment and create a video\n",
        "episode_return, episode_length = 0, 0\n",
        "[state, _] = env.reset()\n",
        "state = prepare_observation_array(state)\n",
        "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
        "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
        "actions = torch.zeros((0, act_dim),  device=device, dtype=torch.long)\n",
        "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
        "print_every = 10\n",
        "iter = 0\n",
        "\n",
        "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
        "for t in range(max_ep_len):\n",
        "    iter += 1\n",
        "    actions = torch.cat([actions, torch.zeros((1, act_dim), dtype=torch.long,  device=device)], dim=0)\n",
        "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
        "\n",
        "    action = get_action(\n",
        "        model,\n",
        "        states,\n",
        "        actions,\n",
        "        rewards,\n",
        "        target_return,\n",
        "        timesteps,\n",
        "    )\n",
        "    \n",
        "    action =   torch.argmax(action).item() # action.detach().cpu().numpy()\n",
        "    \n",
        "    actions[-1] = torch.tensor(action, dtype=torch.long) \n",
        "\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    \n",
        "    if iter % print_every ==0:\n",
        "      image = env.render()\n",
        "      clear_output(wait=True)\n",
        "      plt.imshow(image)\n",
        "      plt.axis('off')  # Hide the axis\n",
        "      display(plt.gcf())\n",
        "    \n",
        "    \n",
        "\n",
        "    state = prepare_observation_array(state)\n",
        "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
        "    states = torch.cat([states, cur_state], dim=0)\n",
        "    rewards[-1] = reward\n",
        "\n",
        "    pred_return = target_return[0, -1] - (reward / scale)\n",
        "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
        "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
        "\n",
        "    episode_return += reward\n",
        "    episode_length += 1\n",
        "\n",
        "    if done:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
